{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7930244,"sourceType":"datasetVersion","datasetId":4661171}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom pyarrow.parquet import ParquetFile\nimport pyarrow as pa \nfrom sklearn.model_selection import train_test_split\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:10:47.275962Z","iopub.execute_input":"2024-03-28T20:10:47.276961Z","iopub.status.idle":"2024-03-28T20:10:51.241958Z","shell.execute_reply.started":"2024-03-28T20:10:47.276923Z","shell.execute_reply":"2024-03-28T20:10:51.240972Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"def get_img_list(df):\n    images=[]\n    for number in range(df.shape[0]):\n        for idx, channels in enumerate(df['X_jets'][number]):\n            for i, row in enumerate(channels):\n                if i==0:\n                    img = row\n                else:\n                    img = np.vstack([img, row])\n            if idx==0:\n                final_img = img\n            else:\n                final_img = np.dstack([final_img, img])\n        images.append(final_img)\n    return images","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:10:51.243960Z","iopub.execute_input":"2024-03-28T20:10:51.244872Z","iopub.status.idle":"2024-03-28T20:10:51.250981Z","shell.execute_reply.started":"2024-03-28T20:10:51.244836Z","shell.execute_reply":"2024-03-28T20:10:51.250018Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def ds_create(paths):\n    for idx,path in enumerate(paths):\n        run = ParquetFile(path) \n        req_rows = next(run.iter_batches(batch_size = 5000)) \n        df = pa.Table.from_batches([req_rows]).to_pandas() \n        images= get_img_list(df)\n        X_dataset = images\n        y_dataset = df['y']\n        print(len(X_dataset), y_dataset.shape)\n        X_train_dataset, X_test_dataset, y_train_dataset, y_test_dataset = train_test_split(X_dataset,y_dataset , random_state=42,test_size=0.2, shuffle=True)\n        if idx==0:\n            X_train = X_train_dataset\n            y_train = y_train_dataset\n            X_test = X_test_dataset\n            y_test = y_test_dataset\n        else:\n            X_train = np.concatenate((X_train,X_train_dataset),axis=0)\n            y_train = np.concatenate((y_train,y_train_dataset),axis=0)\n            X_test = np.concatenate((X_test,X_test_dataset),axis=0)\n            y_test = np.concatenate((y_test,y_test_dataset),axis=0)\n    \n    return X_train,y_train, X_test, y_test","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:10:51.252241Z","iopub.execute_input":"2024-03-28T20:10:51.252564Z","iopub.status.idle":"2024-03-28T20:10:51.267350Z","shell.execute_reply.started":"2024-03-28T20:10:51.252534Z","shell.execute_reply":"2024-03-28T20:10:51.266418Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"path0 = '/kaggle/input/e2e-p2/QCDToGGQQ_IMGjet_RH1all_jet0_run0_n36272.test.snappy.parquet'\npath1 = '/kaggle/input/e2e-p2/QCDToGGQQ_IMGjet_RH1all_jet0_run1_n47540.test.snappy.parquet'\npath2 = '/kaggle/input/e2e-p2/QCDToGGQQ_IMGjet_RH1all_jet0_run2_n55494.test.snappy.parquet'\n\npaths = [path0,path1,path2]\nX_train,y_train, X_test, y_test = ds_create(paths)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:10:51.728527Z","iopub.execute_input":"2024-03-28T20:10:51.728911Z","iopub.status.idle":"2024-03-28T20:12:13.413626Z","shell.execute_reply.started":"2024-03-28T20:10:51.728882Z","shell.execute_reply":"2024-03-28T20:12:13.412507Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"5000 (5000,)\n5000 (5000,)\n5000 (5000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass ImageData(Dataset):\n    def __init__(self, images,labels):\n        self.images = images\n        self.labels = labels\n        self.transform=Compose([ToTensor(),Resize((32,32))])\n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        return self.transform(self.images[idx]), self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:13.415560Z","iopub.execute_input":"2024-03-28T20:12:13.415878Z","iopub.status.idle":"2024-03-28T20:12:13.422376Z","shell.execute_reply.started":"2024-03-28T20:12:13.415852Z","shell.execute_reply":"2024-03-28T20:12:13.421482Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\ntrain_dataloader = DataLoader(ImageData(X_train,y_train), batch_size=batch_size, shuffle=True,)\ntest_dataloader = DataLoader(ImageData(X_test,y_test), batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:13.423713Z","iopub.execute_input":"2024-03-28T20:12:13.423997Z","iopub.status.idle":"2024-03-28T20:12:13.439171Z","shell.execute_reply.started":"2024-03-28T20:12:13.423975Z","shell.execute_reply":"2024-03-28T20:12:13.438172Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class VGG12(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(VGG12, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        \n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n            \n        )\n\n        self.adap_pool_layer = nn.AdaptiveAvgPool2d((7, 7))\n\n        self.linear_layers = nn.Sequential(\n            nn.Linear(in_features=512*7*7, out_features=4096),\n            nn.ReLU(),\n            nn.Dropout2d(0.5),\n            nn.Linear(in_features=4096, out_features=4096),\n            nn.ReLU(),\n            nn.Dropout2d(0.5),\n            nn.Linear(in_features=4096, out_features=self.num_classes)\n        )\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.adap_pool_layer(x)\n        # flatten to prepare for the fully connected layers\n        x = x.view(x.size(0), -1)\n        x = self.linear_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:13.441170Z","iopub.execute_input":"2024-03-28T20:12:13.441503Z","iopub.status.idle":"2024-03-28T20:12:13.455030Z","shell.execute_reply.started":"2024-03-28T20:12:13.441479Z","shell.execute_reply":"2024-03-28T20:12:13.454079Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"channels = 3 \nnum_classes = 1\nnum_epochs = 25\nbatch_size = 64\nlearning_rate = 3e-4\n\n# model = VGG12(channels, num_classes)\nmodel = VGG12(channels,num_classes).to(device)\n\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\ntotal_step = len(train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:13.456064Z","iopub.execute_input":"2024-03-28T20:12:13.456318Z","iopub.status.idle":"2024-03-28T20:12:14.699599Z","shell.execute_reply.started":"2024-03-28T20:12:13.456297Z","shell.execute_reply":"2024-03-28T20:12:14.698817Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:14.700712Z","iopub.execute_input":"2024-03-28T20:12:14.700992Z","iopub.status.idle":"2024-03-28T20:12:14.707445Z","shell.execute_reply.started":"2024-03-28T20:12:14.700968Z","shell.execute_reply":"2024-03-28T20:12:14.706502Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"VGG12(\n  (conv_layers): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU()\n    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU()\n    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): ReLU()\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU()\n    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (19): ReLU()\n    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (adap_pool_layer): AdaptiveAvgPool2d(output_size=(7, 7))\n  (linear_layers): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout2d(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout2d(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def evaluate(loader):\n    model.eval()\n    total_samples = 0\n    correct_samples = 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            pred = model(inputs.float())\n            predicted_labels = (pred.sigmoid().round())\n            correct_samples += (predicted_labels.squeeze(-1) == labels).sum().item()\n            total_samples += labels.shape[0]\n    return correct_samples / total_samples * 100","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:14.708618Z","iopub.execute_input":"2024-03-28T20:12:14.708895Z","iopub.status.idle":"2024-03-28T20:12:14.717780Z","shell.execute_reply.started":"2024-03-28T20:12:14.708873Z","shell.execute_reply":"2024-03-28T20:12:14.716888Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    epoch_loss = 0\n    model.train()\n    for inputs, labels in train_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs.float())\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    acc = evaluate(test_dataloader)\n    print(f'Epoch {epoch}: Loss = {epoch_loss:.4f}, Test Accuracy = {acc:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:12:14.718930Z","iopub.execute_input":"2024-03-28T20:12:14.719224Z","iopub.status.idle":"2024-03-28T20:17:17.041027Z","shell.execute_reply.started":"2024-03-28T20:12:14.719203Z","shell.execute_reply":"2024-03-28T20:17:17.039932Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0: Loss = 130.5154, Test Accuracy = 50.50%\nEpoch 1: Loss = 130.1917, Test Accuracy = 49.50%\nEpoch 2: Loss = 130.3629, Test Accuracy = 50.50%\nEpoch 3: Loss = 130.3562, Test Accuracy = 50.50%\nEpoch 4: Loss = 127.0353, Test Accuracy = 52.90%\nEpoch 5: Loss = 124.4747, Test Accuracy = 60.93%\nEpoch 6: Loss = 123.6765, Test Accuracy = 62.33%\nEpoch 7: Loss = 121.9521, Test Accuracy = 63.37%\nEpoch 8: Loss = 120.0363, Test Accuracy = 62.53%\nEpoch 9: Loss = 118.6060, Test Accuracy = 64.30%\nEpoch 10: Loss = 118.1799, Test Accuracy = 64.63%\nEpoch 11: Loss = 116.6904, Test Accuracy = 63.93%\nEpoch 12: Loss = 115.9402, Test Accuracy = 65.10%\nEpoch 13: Loss = 115.6065, Test Accuracy = 64.97%\nEpoch 14: Loss = 114.3663, Test Accuracy = 64.10%\nEpoch 15: Loss = 112.8168, Test Accuracy = 65.53%\nEpoch 16: Loss = 112.0833, Test Accuracy = 64.10%\nEpoch 17: Loss = 110.9005, Test Accuracy = 64.53%\nEpoch 18: Loss = 109.2873, Test Accuracy = 65.50%\nEpoch 19: Loss = 107.5503, Test Accuracy = 64.67%\nEpoch 20: Loss = 104.4060, Test Accuracy = 64.70%\nEpoch 21: Loss = 100.0588, Test Accuracy = 62.73%\nEpoch 22: Loss = 95.1464, Test Accuracy = 63.50%\nEpoch 23: Loss = 87.9880, Test Accuracy = 61.00%\nEpoch 24: Loss = 80.1808, Test Accuracy = 63.97%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}