{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-29T21:52:26.145163Z","iopub.execute_input":"2024-03-29T21:52:26.146082Z","iopub.status.idle":"2024-03-29T21:52:26.151723Z","shell.execute_reply.started":"2024-03-29T21:52:26.146049Z","shell.execute_reply":"2024-03-29T21:52:26.150593Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nfrom PIL import Image, ImageOps, ImageFilter\nimport torchvision.transforms as transforms\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:10:28.658217Z","iopub.execute_input":"2024-03-29T22:10:28.658581Z","iopub.status.idle":"2024-03-29T22:10:28.664086Z","shell.execute_reply.started":"2024-03-29T22:10:28.658551Z","shell.execute_reply":"2024-03-29T22:10:28.663148Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"'''\nImplementation of Barlow Twins (https://arxiv.org/abs/2103.03230), adapted for ease of use for experiments from\nhttps://github.com/facebookresearch/barlowtwins, with some modifications using code from \nhttps://github.com/lucidrains/byol-pytorch\n'''\n\ndef flatten(t):\n    return t.reshape(t.shape[0], -1)\n\nclass NetWrapper(nn.Module):\n\n    def __init__(self, net, layer = -2):\n        super().__init__()\n        self.net = net\n        self.layer = layer\n\n        self.hidden = None\n        self.hook_registered = False\n\n    def _find_layer(self):\n        if type(self.layer) == str:\n            modules = dict([*self.net.named_modules()])\n            return modules.get(self.layer, None)\n        elif type(self.layer) == int:\n            children = [*self.net.children()]\n            return children[self.layer]\n        return None\n\n    def _hook(self, _, __, output):\n        self.hidden = flatten(output)\n\n    def _register_hook(self):\n        layer = self._find_layer()\n        assert layer is not None, f'hidden layer ({self.layer}) not found'\n        handle = layer.register_forward_hook(self._hook)\n        self.hook_registered = True\n\n    def get_representation(self, x):\n        if self.layer == -1:\n            return self.net(x)\n\n        if not self.hook_registered:\n            self._register_hook()\n\n        _ = self.net(x)\n        hidden = self.hidden\n        self.hidden = None\n        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n        return hidden\n\n    def forward(self, x):\n        representation = self.get_representation(x)\n\n        return representation\n\n\n\ndef off_diagonal(x):\n    # return a flattened view of the off-diagonal elements of a square matrix\n    n, m = x.shape\n    assert n == m\n    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:10:31.936553Z","iopub.execute_input":"2024-03-29T22:10:31.937363Z","iopub.status.idle":"2024-03-29T22:10:31.949682Z","shell.execute_reply.started":"2024-03-29T22:10:31.937326Z","shell.execute_reply":"2024-03-29T22:10:31.948717Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class BarlowTwins(nn.Module):\n    '''\n    Adapted from https://github.com/facebookresearch/barlowtwins for arbitrary backbones, and arbitrary choice of which\n    latent representation to use. Designed for models which can fit on a single GPU (though training can be parallelized\n    across multiple as with any other model). Support for larger models can be done easily for individual use cases by\n    by following PyTorch's model parallelism best practices.\n    '''\n    def __init__(self, backbone, latent_id, projection_sizes, lambd, scale_factor=1):\n        '''\n        \n        \n        :param backbone: Model backbone\n        :param latent_id: name (or index) of the layer to be fed to the projection MLP\n        :param projection_sizes: size of the hidden layers in the projection MLP\n        :param lambd: tradeoff function\n        :param scale_factor: Factor to scale loss by, default is 1\n        '''\n        super().__init__()\n        self.backbone = backbone\n        self.backbone = NetWrapper(self.backbone, latent_id)\n        self.lambd = lambd\n        self.scale_factor = scale_factor\n        # projector\n        sizes = projection_sizes\n        layers = []\n        for i in range(len(sizes) - 2):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n            layers.append(nn.BatchNorm1d(sizes[i + 1]))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n        self.projector = nn.Sequential(*layers)\n\n        # normalization layer for the representations z1 and z2\n        self.bn = nn.BatchNorm1d(sizes[-1], affine=False)\n\n    def forward(self, y1, y2):\n        z1 = self.backbone(y1)\n        z2 = self.backbone(y2)\n        z1 = self.projector(z1)\n        z2 = self.projector(z2)\n\n        # empirical cross-correlation matrix\n        c = torch.mm(self.bn(z1).T, self.bn(z2))\n        c.div_(z1.shape[0])\n\n\n        # use --scale-loss to multiply the loss by a constant factor\n        # see the Issues section of the readme\n        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n        off_diag = off_diagonal(c).pow_(2).sum()\n        loss = self.scale_factor*(on_diag + self.lambd * off_diag)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:10:34.020713Z","iopub.execute_input":"2024-03-29T22:10:34.021297Z","iopub.status.idle":"2024-03-29T22:10:34.033299Z","shell.execute_reply.started":"2024-03-29T22:10:34.021256Z","shell.execute_reply":"2024-03-29T22:10:34.032320Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"'''\n#####\nAdapted from https://github.com/facebookresearch/barlowtwins\n#####\n'''\nclass GaussianBlur(object):\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            sigma = random.random() * 1.9 + 0.1\n            return img.filter(ImageFilter.GaussianBlur(sigma))\n        else:\n            return img\n\n\nclass Solarization(object):\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return ImageOps.solarize(img)\n        else:\n            return img\n\n\nclass Transform:\n    def __init__(self, transform=None, transform_prime=None):\n        '''\n\n        :param transform: Transforms to be applied to first input\n        :param transform_prime: transforms to be applied to second\n        '''\n        if transform == None:\n            self.transform = transforms.Compose([\n                transforms.RandomResizedCrop(224, interpolation=Image.BICUBIC),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n                                            saturation=0.2, hue=0.1)],\n                    p=0.8\n                ),\n                transforms.RandomGrayscale(p=0.2),\n                GaussianBlur(p=1.0),\n                Solarization(p=0.0),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform = transform\n        if transform_prime == None:\n\n            self.transform_prime = transforms.Compose([\n                transforms.RandomResizedCrop(224, interpolation=Image.BICUBIC),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n                                            saturation=0.2, hue=0.1)],\n                    p=0.8\n                ),\n                transforms.RandomGrayscale(p=0.2),\n                GaussianBlur(p=0.1),\n                Solarization(p=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform_prime = transform_prime\n\n    def __call__(self, x):\n        y1 = self.transform(x)\n        y2 = self.transform_prime(x)\n        return y1, y2","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:10:36.203729Z","iopub.execute_input":"2024-03-29T22:10:36.204404Z","iopub.status.idle":"2024-03-29T22:10:36.217741Z","shell.execute_reply.started":"2024-03-29T22:10:36.204370Z","shell.execute_reply":"2024-03-29T22:10:36.216912Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\ndataset = dsets.CIFAR10(root=\"./data\", train=True, transform=Transform(), download=True)\ntotal_size = len(dataset)\ntrain_size = int(0.6 * total_size)\nfine_tune_size = int(0.2 * total_size)\ntest_size = total_size - train_size - fine_tune_size\n\ntrain_dataset, fine_tune_dataset, test_dataset = random_split(dataset, [train_size, fine_tune_size, test_size])\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nprint(\"Unlabeled size:\", len(train_dataset))\nprint(\"Fine-tune size:\", len(fine_tune_dataset))\nprint(\"Test size:\", len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:38:30.742103Z","iopub.execute_input":"2024-03-29T22:38:30.742742Z","iopub.status.idle":"2024-03-29T22:38:31.651283Z","shell.execute_reply.started":"2024-03-29T22:38:30.742714Z","shell.execute_reply":"2024-03-29T22:38:31.650347Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nUnlabeled size: 30000\nFine-tune size: 10000\nTest size: 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"model = torchvision.models.resnet18(zero_init_residual=True)\n    \n\nlearner = BarlowTwins(model, 'avgpool', [512,1024, 1024, 1024],3.9e-3, 1)\n\noptimizer = torch.optim.Adam(learner.parameters(), lr=0.001)\n\n#Single training epoch\ncnt=0\nfor batch_idx, ((x1,x2), _) in enumerate(train_dataloader):\n    loss = learner(x1, x2)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f'Batch {batch_idx} Done')","metadata":{"execution":{"iopub.status.busy":"2024-03-29T23:01:42.015344Z","iopub.execute_input":"2024-03-29T23:01:42.016021Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Batch 0 Done\nBatch 1 Done\nBatch 2 Done\nBatch 3 Done\nBatch 4 Done\nBatch 5 Done\nBatch 6 Done\nBatch 7 Done\nBatch 8 Done\nBatch 9 Done\nBatch 10 Done\nBatch 11 Done\nBatch 12 Done\nBatch 13 Done\nBatch 14 Done\nBatch 15 Done\nBatch 16 Done\nBatch 17 Done\nBatch 18 Done\nBatch 19 Done\nBatch 20 Done\nBatch 21 Done\nBatch 22 Done\nBatch 23 Done\nBatch 24 Done\nBatch 25 Done\nBatch 26 Done\nBatch 27 Done\nBatch 28 Done\nBatch 29 Done\nBatch 30 Done\nBatch 31 Done\nBatch 32 Done\nBatch 33 Done\nBatch 34 Done\nBatch 35 Done\nBatch 36 Done\nBatch 37 Done\nBatch 38 Done\nBatch 39 Done\nBatch 40 Done\nBatch 41 Done\nBatch 42 Done\nBatch 43 Done\nBatch 44 Done\nBatch 45 Done\nBatch 46 Done\nBatch 47 Done\nBatch 48 Done\nBatch 49 Done\nBatch 50 Done\nBatch 51 Done\nBatch 52 Done\nBatch 53 Done\nBatch 54 Done\nBatch 55 Done\nBatch 56 Done\nBatch 57 Done\nBatch 58 Done\nBatch 59 Done\nBatch 60 Done\nBatch 61 Done\nBatch 62 Done\nBatch 63 Done\nBatch 64 Done\nBatch 65 Done\nBatch 66 Done\nBatch 67 Done\nBatch 68 Done\nBatch 69 Done\nBatch 70 Done\nBatch 71 Done\nBatch 72 Done\nBatch 73 Done\nBatch 74 Done\nBatch 75 Done\nBatch 76 Done\nBatch 77 Done\nBatch 78 Done\nBatch 79 Done\nBatch 80 Done\nBatch 81 Done\nBatch 82 Done\nBatch 83 Done\nBatch 84 Done\nBatch 85 Done\nBatch 86 Done\nBatch 87 Done\nBatch 88 Done\nBatch 89 Done\nBatch 90 Done\nBatch 91 Done\nBatch 92 Done\nBatch 93 Done\nBatch 94 Done\nBatch 95 Done\nBatch 96 Done\nBatch 97 Done\nBatch 98 Done\nBatch 99 Done\nBatch 100 Done\nBatch 101 Done\nBatch 102 Done\nBatch 103 Done\nBatch 104 Done\nBatch 105 Done\nBatch 106 Done\nBatch 107 Done\nBatch 108 Done\nBatch 109 Done\nBatch 110 Done\nBatch 111 Done\nBatch 112 Done\nBatch 113 Done\nBatch 114 Done\nBatch 115 Done\nBatch 116 Done\nBatch 117 Done\nBatch 118 Done\nBatch 119 Done\nBatch 120 Done\nBatch 121 Done\nBatch 122 Done\nBatch 123 Done\nBatch 124 Done\nBatch 125 Done\nBatch 126 Done\nBatch 127 Done\nBatch 128 Done\nBatch 129 Done\nBatch 130 Done\nBatch 131 Done\nBatch 132 Done\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    model.train()\n    size = len(dataloader.dataset)\n    running_loss = 0.\n    #total_sample = 0\n    for batch_i, data in enumerate(dataloader):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n#         inputs, labels = data[0], data[1]\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Compute prediction and loss\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        # print statistics\n        running_loss += loss.item()\n        num_batch_print = 200\n        if batch_i % num_batch_print == num_batch_print - 1:\n            current = (batch_i + 1) * len(inputs)\n            print(f'batch: [{batch_i + 1:5d}], loss: {running_loss / num_batch_print:.3f}')\n            print(f'temporary loss: {loss.item():>7f} | [{current:>5d}/{size:>5d}]')\n            running_loss = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_loop(dataloader, model, loss_fn):\n    model.eval()\n    #size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    \n    test_loss, correct = 0, 0\n    total = 0\n    \n    with torch.no_grad():\n        for data in dataloader:\n            # get the inputs for test dataset\n            images, labels = data[0].to(device), data[1].to(device)\n#             images, labels = data[0], data[1]\n\n            \n            # calculate the outputs\n            outputs = model(images)\n            \n            # classify which class the output in\n            _, predicted = torch.max(outputs.data, 1)\n            #_, labels_value = torch.max(labels.data, 1)\n            \n            # obtain the statistics of test loss and correctness\n            test_loss += loss_fn(outputs, labels).item()\n            correct += (predicted == labels).sum().item() \n            #correct += (predicted == labels_value).sum().item() \n            total += labels.size(0)\n\n    print(f\"Test Error: \\n Accuracy: {(100 * correct / total):>0.1f}%\")\n    print(f\"Avg loss: {test_loss / num_batches:>8f} \\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, 2, device=device)\n\nmodel.trainable = True\n\nset_trainable = False\n\nfor layer in model.layers:\n    if layer.name == 'avgpool':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\nfor layer in conv_base.layers:\n    print(layer.name,layer.trainable)\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new\n# criterion = nn.CrossEntropyLoss()\nlr_list = [1e-2]*5\ntime_start = time.time()\n# Pre-trained ResNet training\nfor t in range( len(lr_list) ):\n    print(f\"-------------Epoch {t+1}-------------\")\n    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n    optimizer = optim.Adam(model.parameters(), lr=lr_list[t])\n    train_loop(train_dataloader, model, criterion, optimizer)\n    test_loop(test_dataloader, model, criterion)\nprint(\"Over\")\ntime_end = time.time()\nprint(\"Time Consumption\",time_end-time_start)","metadata":{},"execution_count":null,"outputs":[]}]}